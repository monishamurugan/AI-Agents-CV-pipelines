{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b0a5df1-d55d-46f5-8cfb-2638f4783345",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    set_seed\n",
    ")\n",
    "import transformers\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8606bf48-6414-4aa2-b409-74a164c14a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"./cva_gst_dataset/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53d9bd9a-c2cd-4eb8-ae58-0f9d1d48b854",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612dfb40-7174-4b13-8153-91df5ad9d8b3",
   "metadata": {},
   "source": [
    "##  Evaluate the Model Qualitatively (Human Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68c9285d-7604-4e0b-8bb9-2d81b5669d1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_formatted_prompt(prompt):\n",
    "    return f\"Task: Convert the JSON configuration below into a valid gst-launch-1.0 pipeline command.\\nInput JSON:\\n{prompt}\\nOutput:\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "837d393c-3860-4adb-a5ac-f6ac3b4f1715",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d59d267d-f27a-487f-b5f9-8ae1cb72e21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e9d71a5c4674cc79be535500081c0b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name='microsoft/phi-2'\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                                      device_map='auto',\n",
    "                                                      quantization_config=bnb_config,\n",
    "                                                      trust_remote_code=True,\n",
    "                                                      use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b77b091-5ffb-41c6-8630-fa08b9f8ae6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_text_from_model(model, prompt):\n",
    "    toks = eval_tokenizer(prompt, return_tensors=\"pt\")\n",
    "    res = model.generate(**toks.to(\"cuda\"), max_new_tokens=500, do_sample=True,num_return_sequences=1,temperature=0.1,num_beams=1,top_p=0.95,).to('cpu')\n",
    "    return eval_tokenizer.batch_decode(res,skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47f3bf52-0d52-473e-b7c8-a4b979da0815",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_tokenizer = AutoTokenizer.from_pretrained(model_name, add_bos_token=True, trust_remote_code=True, use_fast=False)\n",
    "eval_tokenizer.pad_token = eval_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27703188-3ec1-4d1a-b472-532cb9c8902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = PeftModel.from_pretrained(base_model, \"peft_GST_pipeline_training_QLora/final-checkpoint/checkpoint-100\",torch_dtype=torch.float16,is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc1d2100-7f9a-47be-9824-0c8c46a29b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "Task: Convert the JSON configuration below into a valid gst-launch-1.0 pipeline command.\n",
      "Input JSON:\n",
      "{\"source_type\": \"file\", \"source_location\": \"/opt/data/retail/storage_7/cam_10.mp4\", \"video_scale\": {\"width\": 1044, \"height\": 958}, \"detect\": {\"model\": \"/var/lib/models/onnx/recognition/5.0/FP16/model_4960.xml\", \"device\": \"HDDL\", \"model_proc\": \"/opt/intel/model_proc/classification/model_9754.json\", \"threshold\": 0.74, \"inference_interval\": 7}, \"track\": {\"tracking_type\": \"short-term-imageless\"}, \"inference\": [{\"model\": \"/usr/local/models/intel/detection/5.0/INT8/model_3984.xml\", \"device\": \"HDDL\", \"inference_region\": \"roi-list\"}, {\"model\": \"/usr/share/models/openvino/classification/2.1/INT8/model_2305.xml\", \"device\": \"CPU\", \"inference_region\": \"roi-list\"}, {\"model\": \"/home/user/.local/models/pytorch/detection/2.1/INT8/model_4294.xml\", \"device\": \"CPU\", \"inference_region\": \"roi-list\"}], \"emit_signals\": false, \"sync\": false, \"drop\": false}\n",
      "Output:\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "GROUND TRUTH:\n",
      "gst-launch-1.0 filesrc location=/opt/data/retail/storage_7/cam_10.mp4 ! decodebin ! videoscale ! video/x-raw,width=1044,height=958 ! gvadetect model=/var/lib/models/onnx/recognition/5.0/FP16/model_4960.xml device=HDDL model-proc=/opt/intel/model_proc/classification/model_9754.json threshold=0.74 inference-interval=7 ! queue ! gvatrack tracking-type=short-term-imageless ! queue ! gvainference model=/usr/local/models/intel/detection/5.0/INT8/model_3984.xml device=HDDL inference-region=roi-list ! queue ! gvainference model=/usr/share/models/openvino/classification/2.1/INT8/model_2305.xml device=CPU inference-region=roi-list ! queue ! gvainference model=/home/user/.local/models/pytorch/detection/2.1/INT8/model_4294.xml device=CPU inference-region=roi-list ! queue ! appsink name=appsink emit-signals=False drop=False sync=False\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "PEFT MODEL:\n",
      "gst-launch-1.0 filesrc location=/opt/data/retail/storage_7/cam_10.mp4! decodebin! videoconvert! videorate! video/x-raw,framerate=1044/958! videoconvert! videoscale! video/x-raw,width=1044,height=958! gvadetect model=/var/lib/models/onnx/recognition/5.0/FP16/model_4960.xml device=HDDL model-proc=/opt/intel/model_proc/classification/model_9754.json threshold=0.74 inference-interval=7! queue! gvatrack tracking-type=short-term-imageless! queue! gvainference model=/usr/local/models/intel/detection/5.0/INT8/model_3984.xml device=HDDL inference-region=roi-list! queue! gvainference model=/usr/share/models/openvino/classification/2.1/INT8/model_2305.xml device=CPU inference-region=roi-list! queue! gvainference model=/home/user/.local/models/pytorch/detection/2.1/INT8/model_4294.xml device=CPU inference-region=roi-list! queue! appsink name=appsink emit-signals=False drop=False sync=False\n",
      "\n",
      "CPU times: user 18.8 s, sys: 227 ms, total: 19.1 s\n",
      "Wall time: 19.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "index = 10\n",
    "\n",
    "sample_prompt = dataset[index]['prompt']\n",
    "sample_pipeline = dataset[index]['pipeline']\n",
    "\n",
    "formatted_prompt = get_formatted_prompt(sample_prompt)\n",
    "\n",
    "peft_model_res = generate_text_from_model(ft_model,formatted_prompt)\n",
    "peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n",
    "output, success, result = peft_model_output.partition('#End')\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{formatted_prompt}\\n')\n",
    "print(dash_line)\n",
    "print(f'GROUND TRUTH:\\n{sample_pipeline}\\n')\n",
    "print(dash_line)\n",
    "print(f'PEFT MODEL:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdd8317-4409-4c22-8381-3cc574b70741",
   "metadata": {},
   "source": [
    "## Evaluate the Model Quantitatively (with ROUGE Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc551624-0d9b-4786-be32-9685f92f54d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dec0bbfc270048678ad30fba2630f08c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                                      device_map='auto',\n",
    "                                                      quantization_config=bnb_config,\n",
    "                                                      trust_remote_code=True,\n",
    "                                                      use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a6102a7-1ce4-4a84-80d8-8dc97e8005da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "prompts = dataset[0:10]['prompt']\n",
    "target_pipelines = dataset[0:10]['pipeline']\n",
    "\n",
    "base_model_pipelines = []\n",
    "peft_model_pipelines = []\n",
    "\n",
    "for idx, prompt in enumerate(prompts):\n",
    "    target_text_output = target_pipelines[idx]\n",
    "    formatted_prompt = get_formatted_prompt(prompt)\n",
    "    \n",
    "    base_model_res = generate_text_from_model(base_model,formatted_prompt)\n",
    "    base_model_text_output = base_model_res[0].split('Output:\\n')[1]\n",
    "    \n",
    "    peft_model_res = generate_text_from_model(ft_model,formatted_prompt)\n",
    "    peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n",
    "\n",
    "    peft_model_text_output, success, result = peft_model_output.partition('#End')\n",
    "    \n",
    "\n",
    "    base_model_pipelines.append(base_model_text_output)\n",
    "    peft_model_pipelines.append(peft_model_text_output)\n",
    "\n",
    "zipped_pipelines = list(zip(target_pipelines, base_model_pipelines, peft_model_pipelines))\n",
    " \n",
    "df = pd.DataFrame(zipped_pipelines, columns = ['target_pipelines', 'base_model_pipelines', 'peft_model_pipelines'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9ad8ab0-07e0-4c18-b1ba-3a8eeb24f5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.7407341611232064, 'rouge2': 0.6067449163123264, 'rougeL': 0.6902516056002185, 'rougeLsum': 0.6899260396914986}\n",
      "PEFT MODEL:\n",
      "{'rouge1': 0.9031376045870323, 'rouge2': 0.8813577342009995, 'rougeL': 0.8885402189318237, 'rougeLsum': 0.8889637696507235}\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "base_model_results = rouge.compute(\n",
    "    predictions=base_model_pipelines,\n",
    "    references=target_pipelines[0:len(base_model_pipelines)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_pipelines,\n",
    "    references=target_pipelines[0:len(peft_model_pipelines)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(base_model_results)\n",
    "print('PEFT MODEL:')\n",
    "print(peft_model_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee952c30-0475-41e5-baba-8fdd323bf6a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\n",
      "rouge1: 16.24%\n",
      "rouge2: 27.46%\n",
      "rougeL: 19.83%\n",
      "rougeLsum: 19.90%\n"
     ]
    }
   ],
   "source": [
    "print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n",
    "\n",
    "improvement = (np.array(list(peft_model_results.values())) - np.array(list(base_model_results.values())))\n",
    "for key, value in zip(peft_model_results.keys(), improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda-default:Python",
   "language": "python",
   "name": "conda-env-.conda-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
